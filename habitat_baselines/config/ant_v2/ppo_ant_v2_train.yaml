# Hyperparameters and ResNet18 from on https://arxiv.org/abs/2012.0611

VERBOSE: False

BASE_TASK_CONFIG_PATH: "configs/tasks/ant-v2.yaml"
TRAINER_NAME: "ppo"
ENV_NAME: "RearrangeRLEnv"
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
#VIDEO_OPTION: []
# Can be uncommented to generate videos.
#VIDEO_OPTION: ["disk", "tensorboard"]
VIDEO_OPTION: ["disk"]
TENSORBOARD_DIR: "data/tb/default/"
VIDEO_DIR: "video_dir"
# Evaluate on all episodes
TEST_EPISODE_COUNT: -1
#NOTE: better to send this from command line
EVAL_CKPT_PATH_DIR: "data/checkpoints/default/"
NUM_ENVIRONMENTS: 16
#use the empty sensor list for training
SENSORS: []
#include the "third_sensor" for eval
#SENSORS: ['THIRD_SENSOR']
CHECKPOINT_FOLDER: "data/checkpoints/default/"
#TOTAL_NUM_STEPS: 75e6
LOG_INTERVAL: 20
NUM_CHECKPOINTS: 100

# Force PyTorch to be single threaded as
# this improves performance considerably
FORCE_TORCH_SINGLE_THREADED: True

RL:
  END_ON_SUCCESS: False
  SUCCESS_REWARD: 0.0
  SLACK_REWARD: 0.0
  POLICY:
      name: "PointNavResNetPolicy"
      action_distribution_type: "gaussian"
  REWARD_MEASURE: "COMPOSITE_ANT_REWARD"
  SUCCESS_MEASURE: "COMPOSITE_ANT_REWARD"
  # GYM_OBS_KEYS: []
  PPO:
    # ppo params
    clip_param: 0.2
    ppo_epoch: 4
    num_mini_batch: 4
    value_loss_coef: 0.5
    entropy_coef: 0.01
    lr: 3e-5
    eps: 1e-5
    max_grad_norm: 0.2
    num_steps: 128 # 16384
    hidden_size: 256
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: False
    reward_window_size: 60

    # Use double buffered sampling, typically helps
    # when environment time is similar or large than
    # policy inference time during rollout generation
    use_double_buffered_sampler: False
